```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
```{r libs}
require('knitr')
require('caret')
require('rpart')
require('rattle')
require('corrplot')
require('gbm')
```
```{r obtain data}
if(!file.exists("pml-training.csv")){
   download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile="pml-training.csv")
}
if(!file.exists("pml-testing.csv")){
    download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile="pml-testing.csv")
}
```

###Background & Introduction
#####Using physical fitness trackers like Fitbit and Nike+ now make it 
possible to collect a large amount of data about personal activity. These 
type of devices are part of the quantified self movement â€“ a group of 
enthusiasts who take measurements about themselves regularly to improve their 
health, to find patterns in their behavior, or because they are tech geeks. 
One thing that people regularly do is quantify how much of a particular 
activity they do, but they rarely quantify how well they do it.

This project attempts to make predictions about the quality of how well an 
exercise might be performed based upon data and research obtained and 
conducted by Pontifical Catholic University of Rio de Janeiro department of 
Informatics and the School of Computing and Communication, Lancaster 
University in the UK.

In this project, we will be to use data from accelerometers on the belt, 
forearm, arm, and dumbell of 6 participant Those participants were asked to 
perform barbell lifts correctly and incorrectly in 5 different ways 
('classe'). The five ways are exactly according to the specification (Class 
A), throwing the elbows to the front (Class B), lifting the dumbbell only 
halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing 
the hips to the front (Class E). Only Class A corresponds to correct 
performance. The goal of this project is to predict the manner in which they 
did the exercise, i.e., Class A to E. More information is available from the 
website here: http://groupware.les.inf.puc-rio.br/har. We are using the whole 
dataset as opposed to the pre-determined training and testing sets provided 
in the course.

####Loading the data
#####We read in the data file accounting for NA-strings and remove columns of 
data that aren't useful for performing any of the analyses we are interested 
in; namely, columns 1-7 are identifying, time series or window variables that 
won't provide use for feature selection in building the model.
```{r load the data}
dat.test <- read.csv('pml-testing.csv', na.strings = c('', 'NA'))
dat.train <- read.csv('pml-training.csv', na.strings = c('', 'NA'))
```

#####Identify columns that have complete cases and only keep those with 
complete cases. The first six columns are not useful for our reproduction nor are the incomplete columns, which are primarily summary statistics.

```{r isolate useful columns}
dat.test.1 <- dat.test[,colnames(dat.test)[complete.cases(t(dat.test))]] 
#ditch NA cols
dat.test.1.1 <- dat.test.1[, 8:60] #remove useless variables (cols 1:7)
new.test <- dat.test.1.1[, 
    !grepl("^amplitude|^kurtosis|^skewness|^avg|^cvtd_timestamp|^max|^min
    |^new_window|^raw_timestamp|^stddev|^var|^user_name|X|^total"
    ,x=names(dat.test.1.1))]
dat.train.1 <- dat.train[,colnames(dat.train)[complete.cases(t(dat.train))]] 
#ditch NA cols
dat.train.1.1 <- dat.train.1[, 8:60] #remove useless variables (cols 1:7)
new.train <- dat.train.1.1[, 
    !grepl("^amplitude|^kurtosis|^skewness|^avg|^cvtd_timestamp|^max|^min
    |^new_window|^raw_timestamp|^stddev|^var|^user_name|X|^total"
    ,x=names(dat.train.1.1))]
rm(dat.test, dat.train, dat.test.1, dat.train.1, dat.test.1.1, dat.train.1.1)
```

```{r setting up the data for model building}
set.seed(3331) #set psuedo-randomization seed for reproducibility
inTrain <- createDataPartition(new.train$classe, p = .70, list = FALSE)
training <- new.train[inTrain, ]
testing <- new.train[-inTrain, ]
m <- cor(training[,1:48])
corrplot(m, method = 'circle')
```
####Model Selection 1 - Generalized Boosted Model
#####We picked a general boosted model as our first model fit attempt to 
predict the quality of how well a user might perfor the dumbbell exercise.

```{r gbm}
if (file.exists('gbmFit1.Rds')) 
    {
      gbmFit1 <- readRDS('gbmFit1.Rds')
} else {
  set.seed(3332) #set psuedo-randomization seed for reproducibility
  fitControl <- trainControl(method = 'cv', number = 3
                            , repeats = 1)
  gbmFit1 <- train(classe ~ .
                   , data = training
                   , method = 'gbm'
                   , trControl = fitControl
                   , verbose = FALSE)
  } 
gbmFit1
g <- ggplot(gbmFit1)
g
```

####Cross Validation and Accuracy of Model One
#####We noted significant accuracy in the training model fit as well as 
significant accuracy in the cross-validation of the testing dataset at ~ 96%, which leaves what appears to be an extremely low out of sample error rating < 5%. The confusion matrix shows a relatively low mis-classification rate on 
the predictions. In general, the misclassifications appeared to be around 1%.

```{r model 1 cv}
pred.gbm <- predict(gbmFit1, testing)
cm.gbm <- confusionMatrix(testing$classe, pred.gbm)
cm.gbm$table;cm.gbm$overall[1]
```

####Model Selection 2 - RPart
#####The gbm performed extremely well and we'll attempt to fit another model 
using a classification tree an cross  validation as a comparison. In this 
approach we'll be using k = 5  folders (or nodes) of classification and 
prediction. What becomes aparenet in the fancy plot is that it appears poor form in the belt_roll predictor either leads directly to a failed performance measure or to throw the forearm predictor off.

```{r r part}
fitControl.2 <- trainControl(method = 'cv', number = 5)
fit.rpart <- train(classe ~ ., data = training, method = "rpart", trControl = fitControl.2)
f <- fancyRpartPlot(fit.rpart$finalModel)
f
```

####Cross validation and accuracy of model 2
#####The rpart model doesn't perform as well with only ~ 49% accuracy or out 
of sample error rate exceeding 51%. Therefore, we wil use model #1 for the final exam.

```{r model 2 cv}
pred.rp <- predict(fit.rpart, testing)
cm.rp <- confusionMatrix(testing$classe, pred.rp)
cm.rp$table;cm.rp$overall[1]
```

```{r PCA work}
#log.dat <- log(abs(new.dat[,1:47])+1) #accounting for -Inf obs by adding 1
#pca.log <- prcomp(log.dat, center = TRUE, scale = TRUE)
#log.dat$classe <- new.dat[,48]
#pca.log$classe <- new.dat[,48]
#plot(pca.log, type = 'l') #Plot of PC variance
```
###Predicting on the Test Set with the Final Model Fit
#####When we run the gbm fit and prediction methods on the testing data we get a 96% accuracy rating for prediction on the testing set. The final output represents the 20 predicted testing results for Quiz #4.

```{r final model fit}
final.fit <- predict(gbmFit1, newdata = new.test)
confusionMatrix(testing$classe, predict(gbmFit1, testing))

final.fit
```
